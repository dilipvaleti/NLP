{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-4918883726d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;31m#########\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 207\u001b[1;33m \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    208\u001b[0m     \u001b[0mmy_tree\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbuild_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[0mprint_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_tree\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'name' is not defined"
     ]
    }
   ],
   "source": [
    "Training_data=[['Green',3,'Mango'],\n",
    "                ['Yellow',3,'Mango'],\n",
    "                ['Red',1,'Grape'],\n",
    "                ['Red',1,'Grape'],\n",
    "                ['Yellow',3,'Lemon']\n",
    "               ]\n",
    "#Column labels\n",
    "header = [\"color\",\"diameter\",\"label\"]\n",
    "\n",
    "def unique_vals(rows,col):\n",
    "    \"\"\"Find the unique values for column in a data set\"\"\"\n",
    "    return set([row[col] for row in rows])\n",
    "#Demo\n",
    "#unnique_vals(Trainind_data,0) then it will return the set of unique items in 1st coulmn\n",
    "def class_counts(rows):\n",
    "    \"\"\"count the each type of example in a data set\"\"\"\n",
    "    counts={}# a dictionary \n",
    "    for row in rows:\n",
    "        #in our data set the last column is always a Label\n",
    "        label=row[-1]\n",
    "        if label not in counts:\n",
    "            counts[label]=0\n",
    "        counts[label]+=1\n",
    "    return counts\n",
    "#Demo\n",
    "#class_count(Training_data)\n",
    "\n",
    "def is_numeric(value):\n",
    "    \"\"\"Test if a alue is numeric\"\"\"\n",
    "    return isinstance(value,int) or isinstance(value,float)\n",
    "#demo\n",
    "#is_numberic(7)\n",
    "\n",
    "class Question:\n",
    "    \"\"\"A question is used to partition a data set.\n",
    "    this class just record the column number and a\n",
    "    column value (e.g. green). The match method is used to compare the features in an\n",
    "    example to the feature values stored in the question.\"\"\"\n",
    "    def __init__(self,column,value):\n",
    "        self.column=column\n",
    "        self.value=value\n",
    "    def match(self,example):\n",
    "        #compare the feature values in the example to the feature values in this question\n",
    "        val=example[self.column]\n",
    "        if is_numeric(val):\n",
    "            return val>=self.value\n",
    "        else:\n",
    "            return val == self.value\n",
    "    def __repr__(self):\n",
    "        # this is just a helper method to print the question in a readable format\n",
    "        condition= '=='\n",
    "        if is_numeric(self.value):\n",
    "            condition = \">=\"\n",
    "        return \"Is %s%s%s\" %( header[self.column],condition,str(self.value))\n",
    "        \n",
    "def partition(rows,question):\n",
    "    \"\"\"Partition a data set\n",
    "    For each row in the data set, check if it match the question, If so\n",
    "    , add it to 'true rows' , otehr wise add it to 'false row'\"\"\"\n",
    "    true_rows,false_rows=[],[]\n",
    "    for row in rows:\n",
    "        if question.match(row):\n",
    "            true_rows.append(row)\n",
    "        else:\n",
    "            false_rows.append(row)\n",
    "        return true_rows, false_rows\n",
    "    \n",
    "#Demo\n",
    "# let's partition the training data based on whether rows are red.\n",
    "#true_rows,false_rows- partition(training_data,Question(0,'Red'))\n",
    "#this will contain the all Red rows assign to #true_rows\n",
    "#this will contain eerythig false assign to #false_rows\n",
    "\n",
    "\n",
    "def gini(rows):\n",
    "    \"\"\"calculate the gini impurity for the list of rows. \"\"\"\n",
    "    counts=class_counts(rows)\n",
    "    impurity=1\n",
    "    for lbl in counts:\n",
    "        prob_of_lbl=counts[lbl]/float(len(rows))\n",
    "        impurity-=prob_of_lbl**2\n",
    "    return impurity\n",
    "def info_gain(left,right,current_uncertainty):\n",
    "    \"\"\" Information gain , the uncertainity of the starting node, minus the \n",
    "    weighted impurity of two child nodes\"\"\"\n",
    "    p=float(len(left))/(len(left)+len(right))\n",
    "    return current_uncertainity-p*gini(left)-(1-p)*ginni(right)\n",
    "\n",
    "def find_best_split(rows):\n",
    "    \"\"\"find the best question to ask by iterating over every feature/ value\n",
    "    and calculate the information gain\"\"\"\n",
    "    best_gain=0 # keep track of the info gain\n",
    "    best_question = None # keep train  of the feature / value that produced it\n",
    "    current_uncertainity=gini(rows)\n",
    "    n_features=len(rows[0])-1 # number of columns\n",
    "    \n",
    "    for col in range(n_features): # for each feature\n",
    "        values=set([row[col] for row in rows])# unique values in the column\n",
    "        \n",
    "        for val in values: # for each alues\n",
    "            question= Question(col,val)\n",
    "            # try spliting the data set\n",
    "            true_rows,false_rows=partition(rows,question)\n",
    "            # skip this split if it doesn't deide the data set\n",
    "            if len(true_rows)==0 or len(false_rows)==0:\n",
    "                continue\n",
    "            # calculate the information gainn form the split\n",
    "            gain= info_gain(true_rows,false_rows,current_unncertainity)\n",
    "            \n",
    "            if gain>= best_gain:\n",
    "                best_gain,best_question=gain,question\n",
    "            \n",
    "        return best_gain, best_question\n",
    "        \n",
    "class leaf:\n",
    "    \"\"\"A leaf node classifies data.\n",
    "    this holds a dictionary of class (e.g. \"mangp\") -> number of times\n",
    "    it appears in the rows from the training data that reach this leaf\"\"\"\n",
    "            \n",
    "    def __init__(self,rows):\n",
    "        self.predictions=class_counts(rows)\n",
    "\n",
    "class Decision_node:\n",
    "    \"\"\" A decision node ask a question, this holds a reference to the question, and to the two child nodes\"\"\"\n",
    "    def __init__(self, question, true_branch, false_branch):\n",
    "        self.question=question\n",
    "        self.true_branch=true_branch\n",
    "        self.false_branch=false_branch\n",
    "        \n",
    "\n",
    "def build_tree(rows):\n",
    "    # try partitioing the dataset on each of the unnique attribute,\n",
    "    #calculate the information gain\n",
    "    #and return the question that produces the highest gain.\n",
    "    gain, question=find_best_split(rows)\n",
    "    \n",
    "    \n",
    "    #Base case: no further info gain\n",
    "    #since we can ask no further questiomns\n",
    "    # we'll return a leaf.\n",
    "    if gain==0:\n",
    "        return leaf(rows)\n",
    "    \n",
    "    #if we reach here, we have found a useful feature/ value\n",
    "    #to partition on\n",
    "    true_rows, false_rows=partition(rows,question)\n",
    "    \n",
    "    #Recursivily built the true branch\n",
    "    true_branch =build_tree(true_rows)\n",
    "    \n",
    "    #Recursivily built the false branch\n",
    "    false_branch =build_tree(false_rows)\n",
    "    \n",
    "    # Return a question node, this records the best feature/value to ask at this point,\n",
    "    #as well as the branches to follow depending on the answer.\n",
    "    return Decision_Node(question,true_branch,false_brach)\n",
    "\n",
    "def print_tree(node,spacing=\"\"):\n",
    "    \"\"\"World most elegent tree printing function \"\"\"\n",
    "    # base case ; we have reached a leaf\n",
    "    if isinstance(node,leaf):\n",
    "        print(spacing +\"Predict\",node.predictions)\n",
    "        return\n",
    "    #print the question at this node\n",
    "    print(spacing + str(node.question))\n",
    "    \n",
    "    #call this function recursivly on the true branch\n",
    "    print(spacing+\"-->True:\")\n",
    "    print_tree(node.true_branch,spacing+\" \")\n",
    "    \n",
    "    #call this funnction recursivvly on the false branch\n",
    "    print(spacing+\"-->False:\")\n",
    "    print_tree(node.false_branch,spacing+\" \")\n",
    "    \n",
    "def classify(row,node):\n",
    "    #data case: we've reached a leaf\n",
    "    if isinstance(node,leaf):\n",
    "        return node.predictions\n",
    "    #Decide whether to follow the true branch or false branch.\n",
    "    #compare the feature / alue stared inn the node,\n",
    "    # to the example we've considering\n",
    "    \n",
    "    if node.question.match(row):\n",
    "        return classify(row,node.true_branch)\n",
    "    else:\n",
    "        return classify(row,node.false_branch)\n",
    "    \n",
    "def print_leaf(counts):\n",
    "    \"\"\"print the predicitons at a leaf\"\"\"\n",
    "    total= sum(counts.values())*1.0\n",
    "    probs={}\n",
    "    for lbl in counts.keys():\n",
    "        probs[lbl]=str(int(counts[lbl]/total*100)) + \"%\"\n",
    "        return probs\n",
    "    \n",
    "#######\n",
    "#Demo:\n",
    "#printing that a bit nicer\n",
    "#print_leaf(classify(trainning_data[0],my_tree))\n",
    "#########\n",
    "#######\n",
    "#Demo:\n",
    "#on the second exmple the confidence is lower\n",
    "#print_leaf(classify(trainning_data[1],my_tree))\n",
    "#########\n",
    "\n",
    "if name == '__main__':\n",
    "    my_tree=build_tree(training_data)\n",
    "    print_tree(my_tree)\n",
    "    \n",
    "    #Evaluate\n",
    "    training_data=[['Green',3,'Mango'],\n",
    "                ['Yellow',4,'Mango'],\n",
    "                ['Red',2,'Grape'],\n",
    "                ['Red',1,'Grape'],\n",
    "                ['Yellow',3,'Lemon']\n",
    "               ]\n",
    "    for row in testing_data:\n",
    "        print(\"Actual : %s, Predicited : %s\"% (row[-1], print_leaf(classify(row,my_tree))))\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
